{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT4AszsTgvPO"
   },
   "source": [
    "# Lab 10: Hierarchical Bayesian Models\n",
    "#### [Penn State Astroinformatics Summer School 2022](https://sites.psu.edu/astrostatistics/astroinfo-su22-program/)\n",
    "#### [Prof. Joel Leja](http://www.personal.psu.edu/jql6565/)\n",
    "#### (Examples based upon the franken-z photometric redshift code by [Dr. Josh Speagle](https://github.com/joshspeagle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6NH1V2KgvPR"
   },
   "source": [
    "A common task in scientific analysis is to characterize a population of objects using data from individual objects which make up that population. For example, you may want to use observations of Earth-analogs to understand how many Earth-like planets exist in the solar system ([Foreman-Mackey et al. 2014](https://ui.adsabs.harvard.edu/abs/2014ApJ...795...64F/abstract)). Or perhaps you want to use far-IR observations of dust emission in molecular clouds to determine their distributions of temperature and emissivity ([Kelly et al. 2012](https://ui.adsabs.harvard.edu/abs/2012ApJ...752...55K/abstract)). The goal doesn't even have to be the characteristics of the population itself, it can instead be the rules governing that population. For example, perhaps your interests are cosmological, and you're interested in combining observations of Type 1a supernovae, correcting each for their local dust environment and host galaxy effects, to infer the values of cosmological parameters such as $\\Omega_m$ and the redshift evolution of dark energy ([Shariff et al. 2016](https://ui.adsabs.harvard.edu/abs/2016ApJ...827....1S/abstract)).\n",
    "\n",
    "A **Hierarchical Bayesian Model** is often an excellent choice for performing these tasks. It can take as inputs full Bayesian posteriors from individual objects. It is possible to directly address challenging components of the modeling, such as biases in the data, target selection function, or correlated measurements. Finally, one can take the population model and re-apply it to individual objects as an *informed prior*, thereby increasing your measurement precision for free! This last effect is known as **hierarchical shrinkage**.\n",
    "\n",
    "We will discuss these one at a time.\n",
    "\n",
    "#### Things we will learn (or review):\n",
    "- Lecture part 1: Population models, priors, sampling\n",
    "  - [Generating Noisy Measurements](#1.-Exploring-Our-Data)\n",
    "  - [Building a Population Model](#2.-Building-a-Population-Model)\n",
    "  - [Defining Priors for Hyper-Parameters](3.-Priors-for-Hyper-Parameters)\n",
    "  - [Sampling our Posteriors](4.-Sampling-Our-Posterior)\n",
    "- Lecture part 2: Hierarchical models\n",
    "  - [Completing the Circle: Hierarchical Shrinkage](5.-Hierarchical-Shrinkage)\n",
    "- Lecture part 3: Adding tools to describe noisy/missing/biased data\n",
    "  - [(Bonus) Modeling Selection Effects](6.-Modeling-Selection-Effects-(Extra-Credit!))\n",
    "  - [(Bonus) Modeling Observational Biases](7.-Modeling-Outliers-(Extra-Credit!))\n",
    "\n",
    "\n",
    "#### Exercises (things you will be able to do!):\n",
    "##### During Class: \n",
    "- Building a population model in a hierarchical framework\n",
    "- Fitting this model to data\n",
    "- Using the population model to improve individual inferences\n",
    "\n",
    "#### And what's the point of the exercises?\n",
    "- Understand how to layer Bayesian models on top of one another using Bayes theorem.\n",
    "- Improve your fits to individual objects by performing hierarchical shrinkage.\n",
    "- Use population models to model complex effects such as selection and data biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "339CuXsQgvPS"
   },
   "source": [
    "# 0. Setup\n",
    "\n",
    "Run the blocks of code below. The first block will import some useful packages and standardize our plot geometry so that everyone's plots should look the same. The second block will define some boilerplate code for your sampling later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dill  # If not running on summer school servers, then you may need to uncomment to install this (and potentially other) packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cdJNqkNgvPT"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "from scipy.special import erf\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot in-line within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# A block of code to specify plotting defaults\n",
    "# run it to standardize your plot geometry!\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'xtick.major.pad': '7.0'})\n",
    "rcParams.update({'xtick.major.size': '7.5'})\n",
    "rcParams.update({'xtick.major.width': '1.5'})\n",
    "rcParams.update({'xtick.minor.pad': '7.0'})\n",
    "rcParams.update({'xtick.minor.size': '3.5'})\n",
    "rcParams.update({'xtick.minor.width': '1.0'})\n",
    "rcParams.update({'ytick.major.pad': '7.0'})\n",
    "rcParams.update({'ytick.major.size': '7.5'})\n",
    "rcParams.update({'ytick.major.width': '1.5'})\n",
    "rcParams.update({'ytick.minor.pad': '7.0'})\n",
    "rcParams.update({'ytick.minor.size': '3.5'})\n",
    "rcParams.update({'ytick.minor.width': '1.0'})\n",
    "rcParams.update({'axes.titlepad': '15.0'})\n",
    "rcParams.update({'font.size': 20})\n",
    "\n",
    "# Science should be repeatable\n",
    "np.random.seed(7001826)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dl3XZpb6gvPU"
   },
   "outputs": [],
   "source": [
    "# Some useful functions for building our models\n",
    "# Run to compile them for later use.\n",
    "# The code comes from the frankenz package, by Josh Speagle\n",
    "# https://github.com/joshspeagle/frankenz\n",
    "\n",
    "# Evaluate a Gaussian PDF averaged over bins of a finite width\n",
    "def gaussian_bin(mu, std, bins):\n",
    "    \"\"\"\n",
    "    Gaussian kernal with mean `mu` and standard deviation `std` evaluated\n",
    "    over a set of bins with edges specified by `bins`.\n",
    "    Returns the PDF integrated over the bins (i.e. an `N - 1`-length vector).\n",
    "    \"\"\"\n",
    "\n",
    "    dif = bins - mu  # difference\n",
    "    y = dif / (np.sqrt(2) * std)  # divide by relative width\n",
    "    cdf = 0.5 * (1. + erf(y))  # CDF evaluated at bin edges\n",
    "    pdf = cdf[1:] - cdf[:-1]  # amplitude integrated over the bins\n",
    "    return pdf\n",
    "\n",
    "# A class for sampling from a population, where the population model\n",
    "# is a histograms with fixed bins of variable height\n",
    "class population_sampler(object):\n",
    "    \"\"\"\n",
    "    Sampler for drawing redshift population distributions given a set of\n",
    "    individual redshift PDFs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pdfs):\n",
    "        \"\"\"\n",
    "        Initialize the sampler.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pdfs : `~numpy.ndarray` of shape `(Nobs, Nbins,)`\n",
    "            The individual redshift PDFs that make up the sample.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize values.\n",
    "        self.pdfs = pdfs\n",
    "        self.samples = []\n",
    "        self.samples_lnp = []\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Re-initialize the sampler.\"\"\"\n",
    "\n",
    "        self.samples = []\n",
    "        self.samples_lnp = []\n",
    "\n",
    "    @property\n",
    "    def results(self):\n",
    "        \"\"\"Return samples.\"\"\"\n",
    "\n",
    "        return np.array(self.samples), np.array(self.samples_lnp)\n",
    "\n",
    "    def run_mcmc(self, Niter, logprior_nz=None, pos_init=None,\n",
    "                 thin=400, mh_steps=3, rstate=None, verbose=True,\n",
    "                 prior_args=[], prior_kwargs={}):\n",
    "        \"\"\"\n",
    "        Sample the distribution using MH-in-Gibbs MCMC.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Niter : int\n",
    "            The number of samples to draw/iterations to run.\n",
    "\n",
    "        logprior_nz : func, optional\n",
    "            A function that returns the ln(prior) on `pos`.\n",
    "\n",
    "        pos_init : `~numpy.ndarray` of shape `(Ndim,)`, optional\n",
    "            The initial position from where we should start sampling.\n",
    "            If not provided, the last position available from the previous\n",
    "            set of samples will be used. If no samples have been drawn, the\n",
    "            initial position will be the stacked PDFs.\n",
    "\n",
    "        thin : int, optional\n",
    "            The number of Gibbs samples (over random pairs) to draw\n",
    "            before saving a sample. Default is `400`.\n",
    "\n",
    "        mh_steps : int, optional\n",
    "            The number of Metropolis-Hastings proposals within each Gibbs\n",
    "            iteration. Default is `3`.\n",
    "\n",
    "        rstate : `~numpy.random.RandomState`\n",
    "            `~numpy.random.RandomState` instance.\n",
    "\n",
    "        verbose : bool, optional\n",
    "            Whether or not to output a simple summary of the current run that\n",
    "            updates with each iteration. Default is `True`.\n",
    "\n",
    "        prior_args : args, optional\n",
    "            Optional arguments for `logprior_nz`.\n",
    "\n",
    "        prior_kwargs : args, optional\n",
    "            Optional keyword arguments for `logprior_nz`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize values.\n",
    "        Nobs, Ndim = self.pdfs.shape\n",
    "        if rstate is None:\n",
    "            rstate = np.random\n",
    "\n",
    "        # Initialize prior.\n",
    "        if logprior_nz is None:\n",
    "            def logprior_nz(pos, *prior_args, **prior_kwargs):\n",
    "                return 0.\n",
    "\n",
    "        # Initialize starting position.\n",
    "        if pos_init is None:\n",
    "            try:\n",
    "                # Try to start from out last position.\n",
    "                pos = self.samples[-1]\n",
    "            except:\n",
    "                # Otherwise, just stack the individual PDFs.\n",
    "                pos = self.pdfs.sum(axis=0) / self.pdfs.sum()\n",
    "                pass\n",
    "        else:\n",
    "            # Use provided position.\n",
    "            pos = pos_init\n",
    "\n",
    "        # Sample.\n",
    "        for i, (x, lnp) in enumerate(self.sample(Niter,\n",
    "                                                 logprior_nz=logprior_nz,\n",
    "                                                 pos_init=pos_init, thin=thin,\n",
    "                                                 mh_steps=mh_steps,\n",
    "                                                 rstate=rstate,\n",
    "                                                 prior_args=prior_args,\n",
    "                                                 prior_kwargs=prior_kwargs)):\n",
    "\n",
    "            self.samples.append(np.array(x))\n",
    "            self.samples_lnp.append(lnp)\n",
    "            if verbose:\n",
    "                sys.stderr.write('\\r Sample {:d}/{:d} [lnpost = {:6.3f}]      '\n",
    "                                 .format(i+1, Niter, lnp))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "    def sample(self, Niter, logprior_nz=None, pos_init=None, thin=400,\n",
    "               mh_steps=3, rstate=None, prior_args=[], prior_kwargs={}):\n",
    "        \"\"\"\n",
    "        Internal generator used for MH-in-Gibbs MCMC sampling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Niter : int\n",
    "            The number of samples to draw/iterations to run.\n",
    "\n",
    "        logprior_nz : func, optional\n",
    "            A function that returns the ln(prior) on `pos`.\n",
    "\n",
    "        pos_init : `~numpy.ndarray` of shape `(Ndim,)`, optional\n",
    "            The initial position from where we should start sampling.\n",
    "            If not provided, the last position available from the previous\n",
    "            set of samples will be used. If no samples have been drawn, the\n",
    "            initial position will be the stacked PDFs.\n",
    "\n",
    "        thin : int, optional\n",
    "            The number of Gibbs samples (over random pairs) to draw\n",
    "            before saving a sample. Default is `400`.\n",
    "\n",
    "        mh_steps : int, optional\n",
    "            The number of Metropolis-Hastings proposals within each Gibbs\n",
    "            iteration. Default is `3`.\n",
    "\n",
    "        rstate : `~numpy.random.RandomState`\n",
    "            `~numpy.random.RandomState` instance.\n",
    "\n",
    "        verbose : bool, optional\n",
    "            Whether or not to output a simple summary of the current run that\n",
    "            updates with each iteration. Default is `True`.\n",
    "\n",
    "        prior_args : args, optional\n",
    "            Optional arguments for `logprior_nz`.\n",
    "\n",
    "        prior_kwargs : args, optional\n",
    "            Optional keyword arguments for `logprior_nz`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize values.\n",
    "        Nobs, Ndim = self.pdfs.shape\n",
    "        if rstate is None:\n",
    "            rstate = np.random\n",
    "\n",
    "        # Initialize prior.\n",
    "        if logprior_nz is None:\n",
    "            def logprior_nz(pos, *prior_args, **prior_kwargs):\n",
    "                return 0.\n",
    "\n",
    "        # Initialize starting position.\n",
    "        if pos_init is None:\n",
    "            pos = self.pdfs.sum(axis=0) / self.pdfs.sum()\n",
    "        else:\n",
    "            pos = pos_init\n",
    "        lnlike, overlap = loglike_nz(pos, self.pdfs, return_overlap=True)\n",
    "        lnprior = logprior_nz(pos, *prior_args, **prior_kwargs)\n",
    "        lnpost = lnlike + lnprior\n",
    "\n",
    "        # Sample.\n",
    "        for i in range(Niter):\n",
    "            # Generate random pairs.\n",
    "            pairs = [rstate.choice(Ndim, size=2, replace=False)\n",
    "                     for i in range(thin)]\n",
    "            # Gibbs step.\n",
    "            for pair in pairs:\n",
    "                # Generate (i, j) basis vector.\n",
    "                t = np.zeros_like(pos)\n",
    "                t[pair] = (1, -1)\n",
    "                # Compute absolute range.\n",
    "                scale = 1e-4 * np.min(np.append(pos[pair], 1. - pos[pair]))\n",
    "                # Compute numerical gradient.\n",
    "                lnp1 = loglike_nz(pos, self.pdfs, overlap=overlap,\n",
    "                                  pair=pair, pair_step=scale/2.)\n",
    "                lnp1 += logprior_nz(pos + t*scale/2.,\n",
    "                                    *prior_args, **prior_kwargs)\n",
    "                lnp2 = loglike_nz(pos, self.pdfs, overlap=overlap,\n",
    "                                  pair=pair, pair_step=-scale/2.)\n",
    "                lnp2 += logprior_nz(pos - t*scale/2.,\n",
    "                                    *prior_args, **prior_kwargs)\n",
    "                grad = (lnp1 - lnp2) / scale\n",
    "                # Rescale so that we're looking at changes in log(post) of ~ 1.\n",
    "                if grad != 0.:\n",
    "                    gscale = min(abs(1. / grad), abs(scale * 1e4))\n",
    "                else:\n",
    "                    gscale = abs(scale)\n",
    "\n",
    "                # Metropolis-Hastings step.\n",
    "                for k in range(mh_steps):\n",
    "                    # Generate proposal.\n",
    "                    z = rstate.randn() * gscale\n",
    "                    # Generate new proposal.\n",
    "                    pos_new = pos + (t * z)\n",
    "                    lnlike_new, overlap_new = loglike_nz(pos_new, self.pdfs,\n",
    "                                                         overlap=overlap,\n",
    "                                                         return_overlap=True,\n",
    "                                                         pair=pair,\n",
    "                                                         pair_step=z)\n",
    "                    lnprior_new = logprior_nz(pos_new,\n",
    "                                              *prior_args, **prior_kwargs)\n",
    "                    lnpost_new = lnlike_new + lnprior_new\n",
    "                    # Metropolis update.\n",
    "                    if -rstate.exponential() < lnpost_new - lnpost:\n",
    "                        pos, lnpost, overlap = pos_new, lnpost_new, overlap_new\n",
    "\n",
    "            # Return current position.\n",
    "            yield pos, lnpost\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhLy594PgvPX"
   },
   "source": [
    "# 1. Exploring Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9st0FfcgvPZ"
   },
   "source": [
    "**Note**: The following examples are based upon the franken-z photometric redshift code by [Dr. Josh Speagle](https://github.com/joshspeagle), a postdoctoral fellow at the University of Toronto.\n",
    "\n",
    "We will begin by loading a set of (pre-generated) *photometric redshifts* from a galaxy population. These are noiseless to begin with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-0FLfDjgvPZ",
    "outputId": "c59ca1ff-af03-4b69-8bc0-fe24b67803c1"
   },
   "outputs": [],
   "source": [
    "redshifts = pickle.load(open('data/mock_sdss_cww_bpz.pkl', 'rb'))  # load data\n",
    "Nobs = len(redshifts)\n",
    "\n",
    "print('Number of observed redshifts:', Nobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOU7Zp5ZgvPa"
   },
   "source": [
    "Now we will simulate observations of these systems by adding a modest amount of Gaussian noise to them. We will assume the mean of each observation is shifted from the true value following this noise distribution, then create an observed probability distribution function (PDF) for each object. While no self-respecting astronomer would do this in practice, we will allow negative redshifts in our data in order to avoid edge effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhB0QFXTgvPa",
    "outputId": "416666f2-7607-47ea-b9d1-1fa3845b1b98"
   },
   "outputs": [],
   "source": [
    "# generate a uniform redshift grid\n",
    "dzgrid = 0.01\n",
    "zgrid = np.arange(-1., 7.+1e-5, dzgrid)\n",
    "\n",
    "# Generate noise values (sigma = 0.05 - 0.2) and simulate new observational means\n",
    "sigma = np.random.uniform(0.05, 0.2, size=Nobs)  # width\n",
    "mu = np.random.normal(redshifts, sigma)  # noisy observation\n",
    "\n",
    "# simulate Nobs observations of varying noise levels (sigma = 0.05 - 0.2)\n",
    "zpdf = np.array([stats.norm.pdf(zgrid, mu[i], sigma[i]) \n",
    "                 for i in range(Nobs)])  # redshift pdfs\n",
    "zpdf /= np.trapz(zpdf, zgrid)[:,None]  # normalizing\n",
    "\n",
    "print('Now we have {0} noisy redshifts on a redshift grid of size {1}.'.format(zpdf.shape[0],zpdf.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boBA3vwFgvPb"
   },
   "source": [
    "Done! Let's take a quick look at our data to make sure everything looks good. We'll plot 6 random PDFs representing our observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3ehiuAMgvPb",
    "outputId": "7cafa112-2284-425f-9684-cd4a8c10d269"
   },
   "outputs": [],
   "source": [
    "# Generate a redshift grid\n",
    "dzbin = 0.1\n",
    "zbins = np.arange(-1, 7.+1e-5, dzbin)  # redshift bins\n",
    "zbins_mid = 0.5 * (zbins[1:] + zbins[:-1])  # bin midpoints\n",
    "Nbins = len(zbins) - 1\n",
    "\n",
    "# plot some PDFs\n",
    "plt.figure(figsize=(20, 12))\n",
    "Nfigs = (2, 3)\n",
    "Nplot = np.prod(Nfigs)\n",
    "colors = plt.get_cmap('viridis')(np.linspace(0., 0.7, Nplot))\n",
    "idxs = np.random.choice(Nobs, size=Nplot)\n",
    "idxs = idxs[np.argsort(redshifts[idxs])]\n",
    "for i, (j, c) in enumerate(zip(idxs, colors)):\n",
    "    plt.subplot(Nfigs[0], Nfigs[1], i + 1)\n",
    "    plt.plot(zgrid, zpdf[j], color=c, lw=4)\n",
    "    plt.vlines(redshifts[j], 0., max(zpdf[j] * 1.2), color='red', \n",
    "               lw=3)\n",
    "    plt.xlim([-0.5, 6])\n",
    "    plt.ylim([0.03, None])\n",
    "    plt.xlabel('Redshift')\n",
    "    plt.yticks([])\n",
    "    plt.ylabel('PDF')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpY5fZZDgvPb"
   },
   "source": [
    "Our data is in place, and indeed, they look noisy! Now let's turn our attention to the population as a whole. What does the redshift distribution look like? How does that compare to our observations? Well for now we'll do the very simplest thing, and stack the PDFs of all of the individual observations. *How bad can this be?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrG-C13agvPb",
    "outputId": "64031d77-c215-44a9-b189-7bb08b34c2dd"
   },
   "outputs": [],
   "source": [
    "# generate redshift bins\n",
    "dzbin = 0.1\n",
    "zbins = np.arange(-1, 7.+1e-5, dzbin)  # redshift bins\n",
    "zbins_mid = 0.5 * (zbins[1:] + zbins[:-1])  # bin midpoints\n",
    "Nbins = len(zbins) - 1\n",
    "\n",
    "# bin up our observed PDFs\n",
    "# We use a special function to do this, which evaluates a Gaussian\n",
    "# PDF over a bin size\n",
    "# Generate the Gaussian PDF, in bins\n",
    "zpdf_bins = np.array([gaussian_bin(mu[i], sigma[i], zbins) \n",
    "                      for i in range(Nobs)])  # redshift pdfs\n",
    "zpdf_bins /= zpdf_bins.sum(axis=1)[:,None] * dzbin  # normalizing\n",
    "# We'll also define a normalized version -- this will be useful later!\n",
    "zpdf_norm = zpdf_bins / zpdf_bins.sum(axis=1)[:, None]\n",
    "\n",
    "\n",
    "# Plot both quantities\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(zgrid, zpdf.sum(axis=0) / Nobs, lw=5, color='blue',\n",
    "         alpha=0.6, label='Stacked PDFs')\n",
    "plt.hist(redshifts, bins=zbins, histtype='step', lw=5,\n",
    "         color='black', alpha=0.7, density=True,label='Truth')\n",
    "plt.hist(zbins_mid, bins=zbins, weights=zpdf_bins.sum(axis=0) / Nobs,\n",
    "         histtype='step', lw=5,\n",
    "         color='blue', alpha=0.5, density=True)\n",
    "plt.xlabel('Redshift')\n",
    "plt.xlim([zgrid[0], zgrid[-1]])\n",
    "plt.yticks([])\n",
    "plt.ylabel('$N(z|\\mathbf{g})$')\n",
    "plt.ylim([0., None])\n",
    "plt.legend(fontsize=28, loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cCEKzfJgvPc"
   },
   "source": [
    "Why don't these two agree? Well, we've added noise! This means that the noisy PDFs are no longer an accurate reconstruction of the true population of galaxies. This should make sense intuitively -- noise *broadens* the overall distribution, so estimating the population redshift distribution $P(z|\\mathbf{g})$ (i.e., the probability of a redshift observation given a population of galaxies $\\mathbf{g}$) requires **deconvolving** the noisy observations.\n",
    "\n",
    "To make further progress we'll have to build a model for our observations. \n",
    "\n",
    "# 2. Building a Population Model\n",
    "Our model for the population should **maximize the posterior probability** according to Bayes' theorem. How will we model the population?\n",
    "\n",
    "For now we will take a simple and flexible approach by modeling the population as a series of redshift **bins** (i.e. a histogram), which can be modeled using a **top-hat kernel** consisting of a product of **Heavyside functions**.\n",
    "\n",
    "Below we define the likelihood for our population model. This is used in conjunction with the above **population sampler** object in order to fit our population model to the data. <span style=\"color:red\"> You will fill in our likelihood -- recall from the lecture how to write down a likelihood for a population model defined by histograms! </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zi_OLqN6gvPc"
   },
   "outputs": [],
   "source": [
    "# We'll have to remove some pieces of this so students can fill in.\n",
    "def loglike_nz(nz, pdfs, overlap=None, return_overlap=False,\n",
    "               pair=None, pair_step=None):\n",
    "    \"\"\"\n",
    "    Compute the log-likelihood for the provided population redshift\n",
    "    distribution `nz` given a collection of PDFs `pdfs`. Assumes that the\n",
    "    distributions both are properly normalized and sum to 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nz : `~numpy.ndarray` of shape `(Nbins,)`\n",
    "        The population redshift distribution.\n",
    "\n",
    "    pdfs : `~numpy.ndarray` of shape `(Nobs, Nbins,)`\n",
    "        The individual redshift PDFs that make up the sample.\n",
    "\n",
    "    overlap : `~numpy.ndarray` of shape `(Nobs,)`\n",
    "        The overlap integrals (sums) between `pdfs` and `nz`. If not provided,\n",
    "        these will be computed.\n",
    "\n",
    "    return_overlap : bool, optional\n",
    "        Whether to return the overlap integrals. Default is `False`.\n",
    "\n",
    "    pair : 2-tuple, optional\n",
    "        A pair of indices `(i, j)` corresponding to a pair of bins that will\n",
    "        be perturbed by `pair_step`.\n",
    "\n",
    "    pair_step : float, optional\n",
    "        The amount by which to perturb the provided pair `(i, j)` in the\n",
    "        `(+, -)` direction, respectively.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loglike : float\n",
    "        The computed log-likelihood.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for negative values.\n",
    "    perturb = 0.\n",
    "    if np.any(~np.isfinite(nz) | (nz < 0.)):\n",
    "        lnlike, overlap = -np.inf, np.zeros(len(pdfs))\n",
    "    else:\n",
    "        # Compute overlap.\n",
    "        if overlap is None:\n",
    "            overlap = np.dot(pdfs, nz)\n",
    "        # Compute perturbation from pair.\n",
    "        if pair is not None:\n",
    "            i, j = pair\n",
    "            if pair_step is not None:\n",
    "                perturb = pair_step * (pdfs[:, i] - pdfs[:, j])\n",
    "        # Compute log-likelihood.\n",
    "        lnlike = np.sum(np.log(overlap + perturb))\n",
    "\n",
    "    if return_overlap:\n",
    "        return lnlike, overlap + perturb\n",
    "    else:\n",
    "        return lnlike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVc0OM-qgvPc"
   },
   "source": [
    "# 3. Priors for Hyper Parameters\n",
    "We will take $P(\\boldsymbol{\\rho})$ to be a **Dirichlet** distribution. Generically, a Dirichlet distribution is a set of numbers between 0 and 1 which all sum up to 1. The Dirichlet distribution is defined by a set \n",
    "\n",
    "$$ \\boldsymbol{\\rho} \\sim {\\rm Dir}\\left(\\mathbf{m} + \\boldsymbol{\\alpha}\\right) $$\n",
    "\n",
    "where $\\boldsymbol{\\alpha} = \\mathbf{1}$ are a set of concentration parameters (with 1 being uniform) and $\\mathbf{m}$ being a set of counts we've previously observed. The concentration parameters determine how evenly distributed the values are: high concentration parameters mean that most of the weight will be in a handful of bins, whereas low concentration numbers mean that we expect them to be evenly distributed. We will come back to this particular choice of prior later when we deal with hierarchical models.\n",
    "\n",
    "Below we'll define our prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sihhoCfJgvPc"
   },
   "outputs": [],
   "source": [
    "# grab representative set of previous redshifts\n",
    "Nref = 1000\n",
    "redshifts_ref = redshifts[-Nref:]\n",
    "alpha = np.ones(Nbins) # the Dirichlet parameter\n",
    "counts_ref, _ = np.histogram(redshifts_ref, bins=zbins)\n",
    "\n",
    "# define our prior\n",
    "def logprior(x, alpha=None, counts_ref=None):\n",
    "    \n",
    "    if alpha is None:\n",
    "        alpha = np.ones_like(x)\n",
    "    if counts_ref is None:\n",
    "        counts_ref = np.zeros_like(x)\n",
    "    if np.any(x < 0.):\n",
    "        return -np.inf\n",
    "\n",
    "    return scipy.stats.dirichlet.logpdf(x, alpha + counts_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VbcVyd8gvPd"
   },
   "source": [
    "# 4. Sampling Our Posterior\n",
    "\n",
    "We now turn to the challenge of generating samples from our distribution. While there are several ways to theoretically do this, we will focus on **Markov Chain Monte Carlo** methods. Due to the constraint that $\\boldsymbol{\\rho}$ must sum to 1, we are sampling from this distribution on the $(N_h - 1)$-dimensional **simplex** since the amplitude of the final bin is always determined by the remaining bins. This creates an additional challenge, since changing one bin will always lead to changes in the other bins. \n",
    "\n",
    "While we could attempt to sample this distribution directly using **Metropolis-Hastings (MH) updates**, given the number of parameters involved in specifying our population distribution $\\boldsymbol{\\rho}$ it is likely better to use **Gibbs sampling** to iterate over conditionals. To satisfy the summation constraint, we opt to use an approach where we update bins $(i, j)$ pairwise so that $i^\\prime + j^\\prime = (i + \\Delta i) + (j + \\Delta j) = i + j \\Rightarrow \\Delta j = -\\Delta i = z$, where $z$ is now our step-size over the bins. We generate proposals for each random pair of bins using MH proposals where the scale is determined adaptively by estimating the gradient for $\\partial/\\partial z$ at each iteration to aim for optimal acceptance fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N66wqELhgvPd",
    "outputId": "2cf5d2e1-e375-46c1-8dc6-007bebcca6d3"
   },
   "outputs": [],
   "source": [
    "# initialize sampler\n",
    "sampler = population_sampler(zpdf_norm)\n",
    "\n",
    "# run MH-in-Gibbs MCMC\n",
    "Nburn = 250\n",
    "Nsamples = 500\n",
    "sampler.run_mcmc(Nsamples + Nburn, logprior_nz=logprior, prior_args=[alpha, counts_ref])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBxUPySLgvPd"
   },
   "source": [
    "Excellent! Now let's compare the results of our population model to our earlier approach of simply stacking the posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8xGaGShgvPd",
    "outputId": "0152ec75-a708-446b-c8a6-6502711d1b9b"
   },
   "outputs": [],
   "source": [
    "# grab samples\n",
    "pdf_samples, pdf_lnps = sampler.results\n",
    "pdf_samples = pdf_samples[-500:] * Nobs  # truncate and rescale\n",
    "\n",
    "# Plot the summed PDFs\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(zgrid, zpdf.sum(axis=0) / Nobs, lw=5, color='blue',\n",
    "         alpha=0.6, label='Stacked PDFs')\n",
    "plt.hist(redshifts, bins=zbins, histtype='step', lw=5,\n",
    "         color='black', alpha=0.7, density=True,label='Truth')\n",
    "plt.hist(zbins_mid, bins=zbins, weights=zpdf_bins.sum(axis=0) / Nobs,\n",
    "         histtype='step', lw=5,\n",
    "         color='blue', alpha=0.5, density=True)\n",
    "plt.xlabel('Redshift')\n",
    "plt.xlim([zgrid[0], zgrid[-1]])\n",
    "plt.yticks([])\n",
    "plt.ylabel('$N(z|\\mathbf{g})$')\n",
    "plt.ylim([0., None])\n",
    "plt.legend(fontsize=28, loc='best')\n",
    "plt.tight_layout()\n",
    "\n",
    "# now plot our population model results!\n",
    "def zplot_bin(samples, label='type', color='blue', downsample=2):\n",
    "    \"\"\"Plot our binned draws.\"\"\"\n",
    "    [plt.hist(zbins_mid + 1e-5, zbins, \n",
    "              weights=samples[i], lw=3,\n",
    "              histtype='step', color=color, \n",
    "              alpha=0.8 if i == 0 else 0.05, # for the legend\n",
    "              label=label if i == 0 else \"\") # for the legend\n",
    "     for i in np.arange(Nsamples)[::downsample]]\n",
    "    h = plt.hist(redshifts, zbins, label='Truth',\n",
    "                 histtype='step', lw=6, color='black', alpha=0.7)\n",
    "    plt.xlabel('Redshift')\n",
    "    plt.xlim([-0.5, 4])\n",
    "    plt.yticks([])\n",
    "    plt.ylim([0, max(h[0]) * 1.2])\n",
    "    plt.ylabel('$N(z|\\mathbf{g})$')\n",
    "    plt.legend(fontsize=26, loc='best')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# plotting\n",
    "plt.subplot(2, 1, 2)\n",
    "zplot_bin(pdf_samples, label='Population', color='darkgoldenrod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71Orl4jvgvPd"
   },
   "source": [
    "# 5. Hierarchical Shrinkage\n",
    "While the above results have looked at the marginalized distribution of $\\boldsymbol{\\rho}$, we can use our results to generate the marginalized distributions for each of our objects $P(z|g)$. Because these apply the prior learned from the population, the individual PDFs are often better-constrained as a result. This effect is known as **hierarchical shrinkage**.\n",
    "\n",
    "This is important -- we're applying our population posterior as a prior for individual objects. Now we understand the results better for individual systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9sY8sHfgvPe",
    "outputId": "8ea7b4cf-da1f-4cc9-9c3d-c8beb41243a2"
   },
   "outputs": [],
   "source": [
    "# plot new PDFs\n",
    "plt.figure(figsize=(20, 12))\n",
    "for i, (j, c) in enumerate(zip(idxs, colors)):\n",
    "    plt.subplot(Nfigs[0], Nfigs[1], i + 1)\n",
    "    n1, _, _ = plt.hist(zbins_mid, zbins, weights=zpdf_bins[j], \n",
    "                        color=c, lw=4, alpha=0.3, histtype='step',\n",
    "                        density=True)\n",
    "    zpdf_bins_t = np.sum([np.random.multinomial(10, (zpdf_bins[j] * p / \n",
    "                                                np.dot(zpdf_bins[j], p)))\n",
    "                          for p in pdf_samples], axis=0)\n",
    "    n2, _, _ = plt.hist(zbins_mid, zbins, weights=zpdf_bins_t, \n",
    "                        color=c, lw=4, alpha=0.8, histtype='step',\n",
    "                        density=True)\n",
    "    plt.vlines(redshifts[j], 0., np.max([n1, n2]) * 1.2, color='red', \n",
    "               lw=3)\n",
    "    plt.xlim([-0.5, 6])\n",
    "    plt.ylim([0.03, None])\n",
    "    plt.xlabel('Redshift')\n",
    "    plt.yticks([])\n",
    "    plt.ylabel('PDF')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJgL2z1igvPe"
   },
   "source": [
    "The plots above show the redshift PDFs for individual objects. The dark lines represent the posteriors after shrinkage, while the light lines represent the original posteriors.\n",
    "\n",
    "Recall that we allowed negative redshifts inferences at the start. However, our population model is 'correct' and does not allow negative redshifts. Look at the first object: we've substantially shrunk the allowed parameter space by applying our hierarchical model!\n",
    "\n",
    "In general, hierarchical shrinkage can either be applied post-fit as we do here, or the fit to the individual objects plus the population sampling can proceed simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9l22wxllgvPe"
   },
   "source": [
    "# 6. Modeling Selection Effects (Extra Credit!)\n",
    "Let's assume that our data has been generated with some non-trivial **selection function**, i.e. we are not observing the full population but instead some subset of it (almost always the case in astronomy!). How do we model this? A good example is in [Foreman-Mackey et al. 2014](https://ui.adsabs.harvard.edu/abs/2014ApJ...795...64F/abstract), Section 5.\n",
    "\n",
    "Here we will assume a detection efficiency of $Q_t = \\frac{n_{obs}}{n_{population}} = \\frac{1}{1+z}$, i.e. objects at higher redshift are harder to find (not unreasonable!). Implement this into our code above and see how our inferred population density changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-0YuhNWgvPe"
   },
   "outputs": [],
   "source": [
    "# Your work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjTPJHXFgvPe"
   },
   "source": [
    "# 7. Modeling Outliers (Extra Credit!)\n",
    "Let's assume that some (small) fraction of our data $f$ are _outliers_, i.e. the inferred redshifts are not consistent with our population model. Below we will step through how to model these by altering the likelihood.\n",
    "\n",
    "- Examine the likelihood for this so-called _mixture model_ in Section 3 of the [classic paper](https://ui.adsabs.harvard.edu/abs/2010arXiv1008.4686H/abstract) _Data analysis recipes: Fitting a model to data_ by David Hogg, Jo Bovy, and Dustin Lang.\n",
    "- Choose some fraction $f$ of your data to be outliers. To these objects, instead add $20\\sigma$ uncertainties.\n",
    "- Rewrite the likelihood function to include the outlier fraction $f$.\n",
    "- See if you can recover the outlier fraction by sampling the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoMOMnysgvPf"
   },
   "outputs": [],
   "source": [
    "# Your work here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "hierarchical_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
