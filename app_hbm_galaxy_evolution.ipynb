{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT4AszsTgvPO"
   },
   "source": [
    "# Lab 11: Hierarchical Bayesian Models\n",
    "#### [Penn State Astroinformatics Summer School 2022](https://sites.psu.edu/astrostatistics/astroinfo-su22-program/)\n",
    "#### [Prof. Joel Leja](http://www.personal.psu.edu/jql6565/)\n",
    "#### (Examples based upon the franken-z photometric redshift code by [Dr. Josh Speagle](https://github.com/joshspeagle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6NH1V2KgvPR"
   },
   "source": [
    "A common task in scientific analysis is to characterize a population of objects using data from individual objects which make up that population. For example, you may want to use observations of Earth-analogs to understand how many Earth-like planets exist in the solar system ([Foreman-Mackey et al. 2014](https://ui.adsabs.harvard.edu/abs/2014ApJ...795...64F/abstract)). Or perhaps you want to use far-IR observations of dust emission in molecular clouds to determine their distributions of temperature and emissivity ([Kelly et al. 2012](https://ui.adsabs.harvard.edu/abs/2012ApJ...752...55K/abstract)). The goal doesn't even have to be the characteristics of the population itself, it can instead be the rules governing that population. For example, perhaps your interests are cosmological, and you're interested in combining observations of Type 1a supernovae, correcting each for their local dust environment and host galaxy effects, to infer the values of cosmological parameters such as $\\Omega_m$ and the redshift evolution of dark energy ([Shariff et al. 2016](https://ui.adsabs.harvard.edu/abs/2016ApJ...827....1S/abstract)).\n",
    "\n",
    "A **Hierarchical Bayesian Model** is often an excellent choice for performing these tasks. It can take as inputs full Bayesian posteriors from individual objects. It is possible to directly address challenging components of the modeling, such as biases in the data, target selection function, or correlated measurements. Finally, one can take the population model and re-apply it to individual objects as an *informed prior*, thereby increasing your measurement precision for free! This last effect is known as **hierarchical shrinkage**.\n",
    "\n",
    "We will discuss these one at a time.\n",
    "\n",
    "#### Things we will learn (or review):\n",
    "- Lecture part 1: Population models, priors, sampling\n",
    "  - [Generating Noisy Measurements](#1.-Exploring-Our-Data)\n",
    "  - [Building a Population Model](#2.-Building-a-Population-Model)\n",
    "  - [Defining Priors for Hyper-Parameters](#3.-Priors-for-Hyper-Parameters)\n",
    "  - [Sampling our Posteriors](#4.-Sampling-Our-Posterior)\n",
    "- Lecture part 2: Hierarchical models\n",
    "  - [Completing the Circle: Hierarchical Shrinkage](5.-Hierarchical-Shrinkage)\n",
    "- Lecture part 3: Adding tools to describe noisy/missing/biased data\n",
    "  - [(Bonus) Tradeoffs In Model Flexibility](#6.-Tradeoffs-in-Model-Flexibility-(Extra-Credit!))\n",
    "  - [(Bonus) Modeling Selection Effects](#7.-Modeling-Selection-Effects-(Extra-Credit!))\n",
    "  - [(Bonus) Modeling Observational Biases](#8.-Modeling-Outliers-(Extra-Credit!))\n",
    "\n",
    "\n",
    "#### Exercises (things you will be able to do!):\n",
    "##### During Class: \n",
    "- Building a population model in a hierarchical framework\n",
    "- Fitting this model to data\n",
    "- Using the population model to improve individual inferences\n",
    "\n",
    "#### And what's the point of the exercises?\n",
    "- Understand how to layer Bayesian models on top of one another using Bayes theorem.\n",
    "- Improve your fits to individual objects by performing hierarchical shrinkage.\n",
    "- Use population models to model complex effects such as selection and data biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "339CuXsQgvPS"
   },
   "source": [
    "# 0. Setup\n",
    "\n",
    "Run the blocks of code below. The first block will import some useful packages and standardize our plot geometry so that everyone's plots should look the same. The second block will define some boilerplate code for your sampling later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dill  # If not running on summer school servers, then you may need to uncomment to install this (and potentially other) packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cdJNqkNgvPT"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "from scipy.special import erf\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot in-line within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# A block of code to specify plotting defaults\n",
    "# run it to standardize your plot geometry!\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'xtick.major.pad': '7.0'})\n",
    "rcParams.update({'xtick.major.size': '7.5'})\n",
    "rcParams.update({'xtick.major.width': '1.5'})\n",
    "rcParams.update({'xtick.minor.pad': '7.0'})\n",
    "rcParams.update({'xtick.minor.size': '3.5'})\n",
    "rcParams.update({'xtick.minor.width': '1.0'})\n",
    "rcParams.update({'ytick.major.pad': '7.0'})\n",
    "rcParams.update({'ytick.major.size': '7.5'})\n",
    "rcParams.update({'ytick.major.width': '1.5'})\n",
    "rcParams.update({'ytick.minor.pad': '7.0'})\n",
    "rcParams.update({'ytick.minor.size': '3.5'})\n",
    "rcParams.update({'ytick.minor.width': '1.0'})\n",
    "rcParams.update({'axes.titlepad': '15.0'})\n",
    "rcParams.update({'font.size': 20})\n",
    "\n",
    "# Science should be repeatable\n",
    "np.random.seed(7001826)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dl3XZpb6gvPU"
   },
   "outputs": [],
   "source": [
    "# Some useful functions for building our models\n",
    "# Run to compile them for later use.\n",
    "# The code comes from the frankenz package, by Josh Speagle\n",
    "# https://github.com/joshspeagle/frankenz\n",
    "\n",
    "# Evaluate a Gaussian PDF averaged over bins of a finite width\n",
    "def gaussian_bin(mu, std, bins):\n",
    "    \"\"\"\n",
    "    Gaussian kernal with mean `mu` and standard deviation `std` evaluated\n",
    "    over a set of bins with edges specified by `bins`.\n",
    "    Returns the PDF integrated over the bins (i.e. an `N - 1`-length vector).\n",
    "    \"\"\"\n",
    "\n",
    "    dif = bins - mu  # difference\n",
    "    y = dif / (np.sqrt(2) * std)  # divide by relative width\n",
    "    cdf = 0.5 * (1. + erf(y))  # CDF evaluated at bin edges\n",
    "    pdf = cdf[1:] - cdf[:-1]  # amplitude integrated over the bins\n",
    "    return pdf\n",
    "\n",
    "# A class for sampling from a population, where the population model\n",
    "# is a histograms with fixed bins of variable height\n",
    "class population_sampler(object):\n",
    "    \"\"\"\n",
    "    Sampler for drawing redshift population distributions given a set of\n",
    "    individual redshift PDFs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pdfs):\n",
    "        \"\"\"\n",
    "        Initialize the sampler.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pdfs : `~numpy.ndarray` of shape `(Nobs, Nbins,)`\n",
    "            The individual redshift PDFs that make up the sample.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize values.\n",
    "        self.pdfs = pdfs\n",
    "        self.samples = []\n",
    "        self.samples_lnp = []\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Re-initialize the sampler.\"\"\"\n",
    "\n",
    "        self.samples = []\n",
    "        self.samples_lnp = []\n",
    "\n",
    "    @property\n",
    "    def results(self):\n",
    "        \"\"\"Return samples.\"\"\"\n",
    "\n",
    "        return np.array(self.samples), np.array(self.samples_lnp)\n",
    "\n",
    "    def run_mcmc(self, Niter, logprior_nz=None, pos_init=None,\n",
    "                 thin=400, mh_steps=3, rstate=None, verbose=True,\n",
    "                 prior_args=[], prior_kwargs={}):\n",
    "        \"\"\"\n",
    "        Sample the distribution using MH-in-Gibbs MCMC.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Niter : int\n",
    "            The number of samples to draw/iterations to run.\n",
    "\n",
    "        logprior_nz : func, optional\n",
    "            A function that returns the ln(prior) on `pos`.\n",
    "\n",
    "        pos_init : `~numpy.ndarray` of shape `(Ndim,)`, optional\n",
    "            The initial position from where we should start sampling.\n",
    "            If not provided, the last position available from the previous\n",
    "            set of samples will be used. If no samples have been drawn, the\n",
    "            initial position will be the stacked PDFs.\n",
    "\n",
    "        thin : int, optional\n",
    "            The number of Gibbs samples (over random pairs) to draw\n",
    "            before saving a sample. Default is `400`.\n",
    "\n",
    "        mh_steps : int, optional\n",
    "            The number of Metropolis-Hastings proposals within each Gibbs\n",
    "            iteration. Default is `3`.\n",
    "\n",
    "        rstate : `~numpy.random.RandomState`\n",
    "            `~numpy.random.RandomState` instance.\n",
    "\n",
    "        verbose : bool, optional\n",
    "            Whether or not to output a simple summary of the current run that\n",
    "            updates with each iteration. Default is `True`.\n",
    "\n",
    "        prior_args : args, optional\n",
    "            Optional arguments for `logprior_nz`.\n",
    "\n",
    "        prior_kwargs : args, optional\n",
    "            Optional keyword arguments for `logprior_nz`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize values.\n",
    "        Nobs, Ndim = self.pdfs.shape\n",
    "        if rstate is None:\n",
    "            rstate = np.random\n",
    "\n",
    "        # Initialize prior.\n",
    "        if logprior_nz is None:\n",
    "            def logprior_nz(pos, *prior_args, **prior_kwargs):\n",
    "                return 0.\n",
    "\n",
    "        # Initialize starting position.\n",
    "        if pos_init is None:\n",
    "            try:\n",
    "                # Try to start from out last position.\n",
    "                pos = self.samples[-1]\n",
    "            except:\n",
    "                # Otherwise, just stack the individual PDFs.\n",
    "                pos = self.pdfs.sum(axis=0) / self.pdfs.sum()\n",
    "                pass\n",
    "        else:\n",
    "            # Use provided position.\n",
    "            pos = pos_init\n",
    "\n",
    "        # Sample.\n",
    "        for i, (x, lnp) in enumerate(self.sample(Niter,\n",
    "                                                 logprior_nz=logprior_nz,\n",
    "                                                 pos_init=pos_init, thin=thin,\n",
    "                                                 mh_steps=mh_steps,\n",
    "                                                 rstate=rstate,\n",
    "                                                 prior_args=prior_args,\n",
    "                                                 prior_kwargs=prior_kwargs)):\n",
    "\n",
    "            self.samples.append(np.array(x))\n",
    "            self.samples_lnp.append(lnp)\n",
    "            if verbose:\n",
    "                sys.stderr.write('\\r Sample {:d}/{:d} [lnpost = {:6.3f}]      '\n",
    "                                 .format(i+1, Niter, lnp))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "    def sample(self, Niter, logprior_nz=None, pos_init=None, thin=400,\n",
    "               mh_steps=3, rstate=None, prior_args=[], prior_kwargs={}):\n",
    "        \"\"\"\n",
    "        Internal generator used for MH-in-Gibbs MCMC sampling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Niter : int\n",
    "            The number of samples to draw/iterations to run.\n",
    "\n",
    "        logprior_nz : func, optional\n",
    "            A function that returns the ln(prior) on `pos`.\n",
    "\n",
    "        pos_init : `~numpy.ndarray` of shape `(Ndim,)`, optional\n",
    "            The initial position from where we should start sampling.\n",
    "            If not provided, the last position available from the previous\n",
    "            set of samples will be used. If no samples have been drawn, the\n",
    "            initial position will be the stacked PDFs.\n",
    "\n",
    "        thin : int, optional\n",
    "            The number of Gibbs samples (over random pairs) to draw\n",
    "            before saving a sample. Default is `400`.\n",
    "\n",
    "        mh_steps : int, optional\n",
    "            The number of Metropolis-Hastings proposals within each Gibbs\n",
    "            iteration. Default is `3`.\n",
    "\n",
    "        rstate : `~numpy.random.RandomState`\n",
    "            `~numpy.random.RandomState` instance.\n",
    "\n",
    "        verbose : bool, optional\n",
    "            Whether or not to output a simple summary of the current run that\n",
    "            updates with each iteration. Default is `True`.\n",
    "\n",
    "        prior_args : args, optional\n",
    "            Optional arguments for `logprior_nz`.\n",
    "\n",
    "        prior_kwargs : args, optional\n",
    "            Optional keyword arguments for `logprior_nz`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize values.\n",
    "        Nobs, Ndim = self.pdfs.shape\n",
    "        if rstate is None:\n",
    "            rstate = np.random\n",
    "\n",
    "        # Initialize prior.\n",
    "        if logprior_nz is None:\n",
    "            def logprior_nz(pos, *prior_args, **prior_kwargs):\n",
    "                return 0.\n",
    "\n",
    "        # Initialize starting position.\n",
    "        if pos_init is None:\n",
    "            pos = self.pdfs.sum(axis=0) / self.pdfs.sum()\n",
    "        else:\n",
    "            pos = pos_init\n",
    "        lnlike, overlap = loglike_nz(pos, self.pdfs, return_overlap=True)\n",
    "        lnprior = logprior_nz(pos, *prior_args, **prior_kwargs)\n",
    "        lnpost = lnlike + lnprior\n",
    "\n",
    "        # Sample.\n",
    "        for i in range(Niter):\n",
    "            # Generate random pairs.\n",
    "            pairs = [rstate.choice(Ndim, size=2, replace=False)\n",
    "                     for i in range(thin)]\n",
    "            # Gibbs step.\n",
    "            for pair in pairs:\n",
    "                # Generate (i, j) basis vector.\n",
    "                t = np.zeros_like(pos)\n",
    "                t[pair] = (1, -1)\n",
    "                # Compute absolute range.\n",
    "                scale = 1e-4 * np.min(np.append(pos[pair], 1. - pos[pair]))\n",
    "                # Compute numerical gradient.\n",
    "                lnp1 = loglike_nz(pos, self.pdfs, overlap=overlap,\n",
    "                                  pair=pair, pair_step=scale/2.)\n",
    "                lnp1 += logprior_nz(pos + t*scale/2.,\n",
    "                                    *prior_args, **prior_kwargs)\n",
    "                lnp2 = loglike_nz(pos, self.pdfs, overlap=overlap,\n",
    "                                  pair=pair, pair_step=-scale/2.)\n",
    "                lnp2 += logprior_nz(pos - t*scale/2.,\n",
    "                                    *prior_args, **prior_kwargs)\n",
    "                grad = (lnp1 - lnp2) / scale\n",
    "                # Rescale so that we're looking at changes in log(post) of ~ 1.\n",
    "                if grad != 0.:\n",
    "                    gscale = min(abs(1. / grad), abs(scale * 1e4))\n",
    "                else:\n",
    "                    gscale = abs(scale)\n",
    "\n",
    "                # Metropolis-Hastings step.\n",
    "                for k in range(mh_steps):\n",
    "                    # Generate proposal.\n",
    "                    z = rstate.randn() * gscale\n",
    "                    # Generate new proposal.\n",
    "                    pos_new = pos + (t * z)\n",
    "                    lnlike_new, overlap_new = loglike_nz(pos_new, self.pdfs,\n",
    "                                                         overlap=overlap,\n",
    "                                                         return_overlap=True,\n",
    "                                                         pair=pair,\n",
    "                                                         pair_step=z)\n",
    "                    lnprior_new = logprior_nz(pos_new,\n",
    "                                              *prior_args, **prior_kwargs)\n",
    "                    lnpost_new = lnlike_new + lnprior_new\n",
    "                    # Metropolis update.\n",
    "                    if -rstate.exponential() < lnpost_new - lnpost:\n",
    "                        pos, lnpost, overlap = pos_new, lnpost_new, overlap_new\n",
    "\n",
    "            # Return current position.\n",
    "            yield pos, lnpost\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhLy594PgvPX"
   },
   "source": [
    "# 1. Exploring Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9st0FfcgvPZ"
   },
   "source": [
    "**Note**: The following examples are based upon the franken-z photometric redshift code by [Dr. Josh Speagle](https://github.com/joshspeagle), a postdoctoral fellow at the University of Toronto.\n",
    "\n",
    "We will begin by loading a set of (pre-generated) *photometric redshifts* from a galaxy population. These are noiseless to begin with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-0FLfDjgvPZ",
    "outputId": "c59ca1ff-af03-4b69-8bc0-fe24b67803c1"
   },
   "outputs": [],
   "source": [
    "redshifts = pickle.load(open('data/mock_sdss_cww_bpz.pkl', 'rb'))  # load data\n",
    "Nobs = len(redshifts)\n",
    "\n",
    "print('Number of observed redshifts:', Nobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOU7Zp5ZgvPa"
   },
   "source": [
    "Now we will simulate observations of these systems by adding a modest amount of Gaussian noise to them. We will assume the mean of each observation is shifted from the true value following this noise distribution, then create an observed probability distribution function (PDF) for each object. While no self-respecting astronomer would do this in practice, we will allow negative redshifts in our data in order to avoid edge effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhB0QFXTgvPa",
    "outputId": "416666f2-7607-47ea-b9d1-1fa3845b1b98"
   },
   "outputs": [],
   "source": [
    "# generate a uniform redshift grid\n",
    "dzgrid = 0.01\n",
    "zgrid = np.arange(-1., 7.+1e-5, dzgrid)\n",
    "\n",
    "# Generate noise values (sigma = 0.05 - 0.2) and simulate new observational means\n",
    "sigma = np.random.uniform(0.1, 0.4, size=Nobs)  # width\n",
    "mu = np.random.normal(redshifts, sigma)  # noisy observation\n",
    "\n",
    "# simulate Nobs observations of varying noise levels (sigma = 0.05 - 0.2)\n",
    "zpdf = np.array([stats.norm.pdf(zgrid, mu[i], sigma[i]) \n",
    "                 for i in range(Nobs)])  # redshift pdfs\n",
    "zpdf /= np.trapz(zpdf, zgrid)[:,None]  # normalizing\n",
    "\n",
    "print('Now we have {0} noisy redshifts on a redshift grid of size {1}.'.format(zpdf.shape[0],zpdf.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boBA3vwFgvPb"
   },
   "source": [
    "Done! Let's take a quick look at our data to make sure everything looks good. We'll plot 6 random PDFs representing our observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3ehiuAMgvPb",
    "outputId": "7cafa112-2284-425f-9684-cd4a8c10d269"
   },
   "outputs": [],
   "source": [
    "# Generate a redshift grid\n",
    "dzbin = 0.1\n",
    "zbins = np.arange(-1, 7.+1e-5, dzbin)  # redshift bins\n",
    "zbins_mid = 0.5 * (zbins[1:] + zbins[:-1])  # bin midpoints\n",
    "Nbins = len(zbins) - 1\n",
    "\n",
    "# plot some PDFs\n",
    "plt.figure(figsize=(20, 12))\n",
    "Nfigs = (2, 3)\n",
    "Nplot = np.prod(Nfigs)\n",
    "colors = plt.get_cmap('viridis')(np.linspace(0., 0.7, Nplot))\n",
    "idxs = np.random.choice(Nobs, size=Nplot)\n",
    "idxs = idxs[np.argsort(redshifts[idxs])]\n",
    "for i, (j, c) in enumerate(zip(idxs, colors)):\n",
    "    plt.subplot(Nfigs[0], Nfigs[1], i + 1)\n",
    "    plt.plot(zgrid, zpdf[j], color=c, lw=4)\n",
    "    plt.vlines(redshifts[j], 0., max(zpdf[j] * 1.2), color='red', \n",
    "               lw=3)\n",
    "    plt.xlim([-0.5, 6])\n",
    "    plt.ylim([0.03, None])\n",
    "    plt.xlabel('Redshift')\n",
    "    plt.yticks([])\n",
    "    plt.ylabel('PDF')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpY5fZZDgvPb"
   },
   "source": [
    "Our data is in place and -- indeed, they look noisy! Now let's turn our attention to the population as a whole. What does the redshift distribution look like? How does that compare to our observations? Well for now we'll do the very simplest thing, and  *How bad can this be?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate redshift bins\n",
    "dzbin = 0.1\n",
    "zbins = np.arange(-1, 7.+1e-5, dzbin)  # redshift bins\n",
    "zbins_mid = 0.5 * (zbins[1:] + zbins[:-1])  # bin midpoints\n",
    "Nbins = len(zbins) - 1\n",
    "\n",
    "# bin up the observations\n",
    "zobs_binned = np.histogram(mu,bins=zbins)\n",
    "\n",
    "# Plot both quantities\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.hist(mu, bins=zbins, histtype='step', lw=5,\n",
    "         color='blue', alpha=0.7, density=True,\n",
    "         label='Histogram of observed redshifts')\n",
    "plt.hist(redshifts, bins=zbins, histtype='step', lw=5,\n",
    "         color='black', alpha=0.7, density=True,label='Truth')\n",
    "plt.xlabel('Redshift')\n",
    "plt.xlim([zgrid[0], zgrid[-1]])\n",
    "plt.yticks([])\n",
    "plt.ylabel('$N(z|\\mathbf{g})$')\n",
    "plt.ylim([0., None])\n",
    "plt.legend(fontsize=28, loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm -- why did this fail? **Spend some time thinking about it. Try to identify three problems with this histogram before moving on.**\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "Answers: the redshift distribution goes below zero, the peaks do not match, and the observed distribution is wider than the true distribution. Hmm. What are we missing? Perhaps it will look better if we include the uncertainties! Let's try the next simplest thing: stacking the PDFs of all of the individual observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrG-C13agvPb",
    "outputId": "64031d77-c215-44a9-b189-7bb08b34c2dd"
   },
   "outputs": [],
   "source": [
    "# bin up our observed PDFs\n",
    "# We use a special function to do this, which evaluates a Gaussian\n",
    "# PDF over a bin size\n",
    "# Generate the Gaussian PDF, in bins\n",
    "zpdf_bins = np.array([gaussian_bin(mu[i], sigma[i], zbins) \n",
    "                      for i in range(Nobs)])  # redshift pdfs\n",
    "zpdf_bins /= zpdf_bins.sum(axis=1)[:,None] * dzbin  # normalizing\n",
    "# We'll also define a normalized version -- this will be useful later!\n",
    "zpdf_norm = zpdf_bins / zpdf_bins.sum(axis=1)[:, None]\n",
    "\n",
    "\n",
    "# Plot both quantities\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(zgrid, zpdf.sum(axis=0) / Nobs, lw=5, color='blue',\n",
    "         alpha=0.6, label='Stacked PDFs')\n",
    "plt.hist(redshifts, bins=zbins, histtype='step', lw=5,\n",
    "         color='black', alpha=0.7, density=True,label='Truth')\n",
    "plt.hist(zbins_mid, bins=zbins, weights=zpdf_bins.sum(axis=0) / Nobs,\n",
    "         histtype='step', lw=5,\n",
    "         color='blue', alpha=0.5, density=True)\n",
    "plt.xlabel('Redshift')\n",
    "plt.xlim([zgrid[0], zgrid[-1]])\n",
    "plt.yticks([])\n",
    "plt.ylabel('$N(z|\\mathbf{g})$')\n",
    "plt.ylim([0., None])\n",
    "plt.legend(fontsize=28, loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cCEKzfJgvPc"
   },
   "source": [
    "Why don't these two agree? Well, we've added noise! This means that the noisy PDFs are no longer an accurate reconstruction of the true population of galaxies. This should make sense intuitively -- noise *broadens* the overall distribution, so estimating the population redshift distribution $P(z|\\mathbf{g})$ (i.e., the probability of a redshift observation given a population of galaxies $\\mathbf{g}$) requires **deconvolving** the noisy observations.\n",
    "\n",
    "To make further progress we'll have to build a model for our observations. \n",
    "\n",
    "# 2. Building a Population Model\n",
    "Our model for the population should **maximize the posterior probability** according to Bayes' theorem. How will we model the population?\n",
    "\n",
    "For now we will take a simple and flexible approach by modeling the population as a series of redshift **bins** (i.e. a histogram), which can be modeled using a **top-hat kernel** consisting of a product of [**Heaviside functions**](https://en.wikipedia.org/wiki/Heaviside_step_function). This means that for a set of N observed probability distribution functions for redshift, where a single observation is denoted by $P_n(z)$, and a modeled distribution of true redshifts $N(z)$, the likelihood function is:\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L} = \\sum_0^N \\ln (P_n(z) \\cdot N(z) )\n",
    "$$\n",
    "\n",
    "Below we define the likelihood for our population model. This is used in conjunction with the above **population sampler** object in order to fit our population model to the data.\n",
    "\n",
    "Note that we allow some additional functionality in the below function, where we can update a specified set of bins $(i, j)$ pairwise, which will slightly modify the likelihood. This is important for our chosen method of sampling from the distribution; you can ignore it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zi_OLqN6gvPc"
   },
   "outputs": [],
   "source": [
    "# Likelihood for a model defined by bins\n",
    "def loglike_nz(nz, pdfs, overlap=None, return_overlap=False,\n",
    "               pair=None, pair_step=None):\n",
    "    \"\"\"\n",
    "    Compute the log-likelihood for the provided population redshift\n",
    "    distribution `nz` given a collection of PDFs `pdfs`. Assumes that the\n",
    "    distributions both are properly normalized and sum to 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nz : `~numpy.ndarray` of shape `(Nbins,)`\n",
    "        The population redshift distribution.\n",
    "\n",
    "    pdfs : `~numpy.ndarray` of shape `(Nobs, Nbins,)`\n",
    "        The individual redshift PDFs that make up the sample.\n",
    "\n",
    "    overlap : `~numpy.ndarray` of shape `(Nobs,)`\n",
    "        The overlap integrals (sums) between `pdfs` and `nz`. If not provided,\n",
    "        these will be computed.\n",
    "\n",
    "    return_overlap : bool, optional\n",
    "        Whether to return the overlap integrals. Default is `False`.\n",
    "\n",
    "    pair : 2-tuple, optional\n",
    "        A pair of indices `(i, j)` corresponding to a pair of bins that will\n",
    "        be perturbed by `pair_step`.\n",
    "\n",
    "    pair_step : float, optional\n",
    "        The amount by which to perturb the provided pair `(i, j)` in the\n",
    "        `(+, -)` direction, respectively.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loglike : float\n",
    "        The computed log-likelihood.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for negative values.\n",
    "    perturb = 0.\n",
    "    if np.any(~np.isfinite(nz) | (nz < 0.)):\n",
    "        lnlike, overlap = -np.inf, np.zeros(len(pdfs))\n",
    "    else:\n",
    "        # Compute overlap.\n",
    "        if overlap is None:\n",
    "            overlap = np.dot(pdfs, nz)\n",
    "        # Compute perturbation from pair.\n",
    "        if pair is not None:\n",
    "            i, j = pair\n",
    "            if pair_step is not None:\n",
    "                perturb = pair_step * (pdfs[:, i] - pdfs[:, j])\n",
    "        # Compute log-likelihood.\n",
    "        lnlike = np.sum(np.log(overlap + perturb))\n",
    "\n",
    "    if return_overlap:\n",
    "        return lnlike, overlap + perturb\n",
    "    else:\n",
    "        return lnlike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVc0OM-qgvPc"
   },
   "source": [
    "# 3. Priors for Hyper Parameters\n",
    "We will take $P(\\boldsymbol{\\rho})$ to be a **Dirichlet** distribution. Generically, a Dirichlet distribution is a set of numbers between 0 and 1 which all sum up to 1. The Dirichlet distribution is defined by a set \n",
    "\n",
    "$$ \\boldsymbol{\\rho} \\sim {\\rm Dir}\\left(\\mathbf{m} + \\boldsymbol{\\alpha}\\right) $$\n",
    "\n",
    "where $\\boldsymbol{\\alpha} = \\mathbf{1}$ are a set of concentration parameters (with 1 being uniform) and $\\mathbf{m}$ being a set of counts we've previously observed. The concentration parameters determine how evenly distributed the values are: high concentration parameters mean that most of the weight will be in a handful of bins, whereas low concentration numbers mean that we expect them to be evenly distributed. \n",
    "\n",
    "Below we'll define our prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sihhoCfJgvPc"
   },
   "outputs": [],
   "source": [
    "# grab representative set of previous redshifts\n",
    "Nref = 1000\n",
    "redshifts_ref = redshifts[-Nref:]\n",
    "alpha = np.ones(Nbins) # the Dirichlet parameter\n",
    "counts_ref, _ = np.histogram(redshifts_ref, bins=zbins)\n",
    "\n",
    "# define our prior\n",
    "def logprior(x, alpha=None, counts_ref=None):\n",
    "    \n",
    "    if alpha is None:\n",
    "        alpha = np.ones_like(x)\n",
    "    if counts_ref is None:\n",
    "        counts_ref = np.zeros_like(x)\n",
    "    if np.any(x < 0.):\n",
    "        return -np.inf\n",
    "\n",
    "    return scipy.stats.dirichlet.logpdf(x, alpha + counts_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we illustrate the effect of choosing different concentration parameters. Effectively, as the concentration _increases_, the histograms become more _evenly distributed_. It's important to understand how your priors are set -- if our data aren't very informative, the choice of hyperparameter prior can be very relevant! Note that we choose $\\alpha=1$ as our prior for the rest of the workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we simulate a histogram with 10 bins, 10 times,\n",
    "# with several different choices for alpha\n",
    "def simulate_histogram(alpha,n_histogram=10,ndraw=10):\n",
    "    draws = scipy.stats.dirichlet.rvs([alpha]*n_histogram, size=ndraw, random_state=1)\n",
    "    for draw in draws:\n",
    "        plt.plot(draw, lw=2, color='black',alpha=0.6,drawstyle='steps-mid')\n",
    "    plt.xlabel('bin number')\n",
    "    plt.title(r'$\\alpha$='+str(alpha))\n",
    "    plt.ylim(0,1)\n",
    "\n",
    "plt.figure(1)\n",
    "simulate_histogram(0.2)\n",
    "plt.figure(2)\n",
    "simulate_histogram(1.0)\n",
    "plt.figure(3)\n",
    "simulate_histogram(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VbcVyd8gvPd"
   },
   "source": [
    "# 4. Sampling Our Posterior\n",
    "\n",
    "We now turn to the challenge of generating samples from our distribution. While there are several ways to theoretically do this, we will focus on **Markov Chain Monte Carlo** methods. Due to the constraint that $\\boldsymbol{\\rho}$ must sum to 1, we are sampling from this distribution on the $(N_h - 1)$-dimensional **simplex** since the amplitude of the final bin is always determined by the remaining bins. This creates an additional challenge, since changing one bin will always lead to changes in the other bins. \n",
    "\n",
    "While we could attempt to sample this distribution directly using **Metropolis-Hastings (MH) updates**, given the number of parameters involved in specifying our population distribution $\\boldsymbol{\\rho}$ it is likely better to use **Gibbs sampling** to iterate over conditionals. To satisfy the summation constraint, we opt to use an approach where we update bins $(i, j)$ pairwise so that $i^\\prime + j^\\prime = (i + \\Delta i) + (j + \\Delta j) = i + j \\Rightarrow \\Delta j = -\\Delta i = x$, where $x$ is now our step-size over the bins. We generate proposals for each random pair of bins using MH proposals where the scale is determined adaptively by estimating the gradient for $\\partial/\\partial x$ at each iteration to aim for optimal acceptance fractions.\n",
    "\n",
    "Note that this is only _one_ approach to efficiently sample from a high-dimensional model. It has the drawback of having to specify the conditionals, which can take a significant amount of human development time (and is error-prone!). Another option is to use [**Hamiltonian Monte Carlo**](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo) (HMC) sampling, particularly the popular No-U-Turn Sampler (NUTS), which uses gradients to efficiently sample from high-dimensional models. Until recently this used to require being able to write down analytic gradients for your model, which is also time-consuming and error-prone. Now, with the rise of efficient autodifferentiation software, this step can often be replaced with a black-box autodiff software. If you want to sample from a high-dimensional model but do not want to calculate conditionals, HMC can be an excellent choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N66wqELhgvPd",
    "outputId": "2cf5d2e1-e375-46c1-8dc6-007bebcca6d3"
   },
   "outputs": [],
   "source": [
    "# initialize sampler\n",
    "sampler = population_sampler(zpdf_norm)\n",
    "\n",
    "# run MH-in-Gibbs MCMC\n",
    "Nburn = 250\n",
    "Nsamples = 500\n",
    "sampler.run_mcmc(Nsamples + Nburn, logprior_nz=logprior, prior_args=[alpha, counts_ref])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBxUPySLgvPd"
   },
   "source": [
    "Excellent! Now let's compare the results of our population model to our earlier approach of simply stacking the posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8xGaGShgvPd",
    "outputId": "0152ec75-a708-446b-c8a6-6502711d1b9b"
   },
   "outputs": [],
   "source": [
    "# grab samples\n",
    "pdf_samples, pdf_lnps = sampler.results\n",
    "pdf_samples = pdf_samples[-500:] * Nobs  # truncate and rescale\n",
    "\n",
    "# Plot the summed PDFs\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(zgrid, zpdf.sum(axis=0) / Nobs, lw=5, color='blue',\n",
    "         alpha=0.6, label='Stacked PDFs')\n",
    "plt.hist(redshifts, bins=zbins, histtype='step', lw=5,\n",
    "         color='black', alpha=0.7, density=True,label='Truth')\n",
    "plt.hist(zbins_mid, bins=zbins, weights=zpdf_bins.sum(axis=0) / Nobs,\n",
    "         histtype='step', lw=5,\n",
    "         color='blue', alpha=0.5, density=True)\n",
    "plt.xlabel('Redshift')\n",
    "plt.xlim([zgrid[0], zgrid[-1]])\n",
    "plt.yticks([])\n",
    "plt.ylabel('$N(z|\\mathbf{g})$')\n",
    "plt.ylim([0., None])\n",
    "plt.legend(fontsize=28, loc='best')\n",
    "plt.tight_layout()\n",
    "\n",
    "# now plot our population model results!\n",
    "def zplot_bin(samples, label='type', color='blue', downsample=2):\n",
    "    \"\"\"Plot our binned draws.\"\"\"\n",
    "    [plt.hist(zbins_mid + 1e-5, zbins, \n",
    "              weights=samples[i], lw=3,\n",
    "              histtype='step', color=color, \n",
    "              alpha=0.8 if i == 0 else 0.05, # for the legend\n",
    "              label=label if i == 0 else \"\") # for the legend\n",
    "     for i in np.arange(Nsamples)[::downsample]]\n",
    "    h = plt.hist(redshifts, zbins, label='Truth',\n",
    "                 histtype='step', lw=6, color='black', alpha=0.7)\n",
    "    plt.xlabel('Redshift')\n",
    "    plt.xlim([-0.5, 4])\n",
    "    plt.yticks([])\n",
    "    plt.ylim([0, max(h[0]) * 1.2])\n",
    "    plt.ylabel('$N(z|\\mathbf{g})$')\n",
    "    plt.legend(fontsize=26, loc='best')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# plotting\n",
    "plt.subplot(2, 1, 2)\n",
    "zplot_bin(pdf_samples, label='Population', color='darkgoldenrod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks **significantly** better! Our model is now centered on the truth, and is able to reproduce some properties of the distribution which the stacked PDFs simply failed to find (e.g., that there are no -- or very few -- negative redshifts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71Orl4jvgvPd"
   },
   "source": [
    "# 5. Hierarchical Shrinkage\n",
    "While the above results have looked at the marginalized distribution of $\\boldsymbol{\\rho}$, we can use our results to generate the marginalized distributions for each of our objects $P(z|g)$. Because these apply the prior learned from the population, the individual PDFs are often better-constrained as a result. This effect is known as **hierarchical shrinkage**.\n",
    "\n",
    "This is important -- we're applying our population posterior as a prior for individual objects. This means that, by using a population model as a constraint, we now have produced more precise results for individual objects! We are effectively saying that each object should behave like the population of objects -- which is often (but not always!) a good assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9sY8sHfgvPe",
    "outputId": "8ea7b4cf-da1f-4cc9-9c3d-c8beb41243a2"
   },
   "outputs": [],
   "source": [
    "# plot new PDFs\n",
    "plt.figure(figsize=(20, 12))\n",
    "for i, (j, c) in enumerate(zip(idxs, colors)):\n",
    "    plt.subplot(Nfigs[0], Nfigs[1], i + 1)\n",
    "    n1, _, _ = plt.hist(zbins_mid, zbins, weights=zpdf_bins[j], \n",
    "                        color=c, lw=4, alpha=0.3, histtype='step',\n",
    "                        density=True)\n",
    "    zpdf_bins_t = np.sum([np.random.multinomial(10, (zpdf_bins[j] * p / \n",
    "                                                np.dot(zpdf_bins[j], p)))\n",
    "                          for p in pdf_samples], axis=0)\n",
    "    n2, _, _ = plt.hist(zbins_mid, zbins, weights=zpdf_bins_t, \n",
    "                        color=c, lw=4, alpha=0.8, histtype='step',\n",
    "                        density=True)\n",
    "    plt.vlines(redshifts[j], 0., np.max([n1, n2]) * 1.2, color='red', \n",
    "               lw=3)\n",
    "    plt.xlim([-0.5, 6])\n",
    "    plt.ylim([0.03, None])\n",
    "    plt.xlabel('Redshift')\n",
    "    plt.yticks([])\n",
    "    plt.ylabel('PDF')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJgL2z1igvPe"
   },
   "source": [
    "The plots above show the redshift PDFs for individual objects. The dark lines represent the posteriors after shrinkage, while the light lines represent the original posteriors. What do you think -- does it look better? It should!\n",
    "\n",
    "Recall that we allowed negative redshifts inferences at the start. However, our population model is 'correct' and does not allow negative redshifts. Look at the first object: we've substantially shrunk the allowed parameter space by applying our hierarchical model!\n",
    "\n",
    "In general, hierarchical shrinkage can either be applied post-fit as we do here, or the fit to the individual objects plus the population sampling can proceed simultaneously.\n",
    "\n",
    "The shrinkage effect has improved our estimates for individual object redshifts. How much? Let's compute the standard deviation in the posterior for a few objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the PDF widths, before and after shrinkage\n",
    "idxs = np.random.choice(Nobs, size=20)\n",
    "print('posterior widths (original vs shrunk)')\n",
    "mean = []\n",
    "for j in idxs:\n",
    "    zpdf_bins_t = np.sum([np.random.multinomial(10, (zpdf_bins[j] * p / \n",
    "                                                np.dot(zpdf_bins[j], p)))\n",
    "                          for p in pdf_samples], axis=0)\n",
    "    original_std = np.sqrt(np.cov(zbins_mid, aweights=zpdf_bins[j]))\n",
    "    shrunk_std = np.sqrt(np.cov(zbins_mid, aweights=zpdf_bins_t))\n",
    "    mean += [original_std - shrunk_std]\n",
    "    print('{0:.2f},{1:.2f}'.format(original_std,shrunk_std))\n",
    "\n",
    "print('average posterior standard deviation decreased by {0:.2f}'.format(np.mean(mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So, knowing the population distribution really _is_ useful here. **Pause for a moment to think** -- how would you expect this ratio would change if we were to increase/decrease the number of objects in the sample? What about if we were to increase or decrease the precision of each object's redshift?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "In general, the effect of shrinkage should **increase** if there are more objects in the sample, since we will know the population distribution better. And it should also **increase** if we are looking at an object with a more uncertain redshift -- if the data aren't very informative about the answer, the relative importance of the priors (i.e. the population model) will increase!\n",
    "\n",
    "And -- let's also check the bias. Does that shrink as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we check the bias\n",
    "print('bias (original vs shrunk)')\n",
    "original_mean_bias = []\n",
    "shrunk_mean_bias = []\n",
    "for j in idxs:\n",
    "    zpdf_bins_t = np.sum([np.random.multinomial(10, (zpdf_bins[j] * p / \n",
    "                                                np.dot(zpdf_bins[j], p)))\n",
    "                          for p in pdf_samples], axis=0)\n",
    "    original_bias = np.average(zbins_mid, weights=zpdf_bins[j,:])-redshifts[j]\n",
    "    shrunk_bias = np.average(zbins_mid, weights=zpdf_bins_t)-redshifts[j]\n",
    "    original_mean_bias += [original_bias]\n",
    "    shrunk_mean_bias += [shrunk_bias]\n",
    "    print('{0:.2f},{1:.2f}'.format(original_bias,shrunk_bias))\n",
    "\n",
    "print('average bias was originally {0:.2f}'.format(np.mean(original_mean_bias)))\n",
    "print('after shrinkage, average bias is now {0:.2f}'.format(np.mean(shrunk_mean_bias)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So**: did the bias in individual redshift inferences decrease after hierarchical shrinkage? Why or why not? How do you think this effect (if there is one!) might scale with the sample size?\n",
    "\n",
    "Congratulations on running a hierarchical model! Now it's time to get out there and do some data crunching! But before you go, there are some extra credit assignments and questions below to help solidify and deepen your knowledge of Bayesian Hierarchical Modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Tradeoffs in Model Flexibility (Extra Credit!)\n",
    "We've been using a population model with a fixed number of bins. How would our results change as we change the number of bins? If we **increased** the number of bins, would our uncertainties go up or down? Try it out below (or just ruminate on it instead!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you should find that as you increase the number of bins, the uncertainty on any given bin will also increase. This is because smaller bins means fewer objects in each bin, and thus looser constraints on the height of the bins. This forces a tradeoff: we have to choose between **higher redshift resolution** and **lower uncertainties**!\n",
    "\n",
    "There is a way around this tradeoff, known as [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)). This effectively enforces a smoothness prior, allowing us to create posterior histograms with many bins that are both smooth and low variance.Â But be careful -- you shouldn't impose regularization if you do expect there to be real, sharp features in the data (in the case of large-scale structure, this might well be expected due to [cosmic variance](https://en.wikipedia.org/wiki/Cosmic_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9l22wxllgvPe"
   },
   "source": [
    "# 7. Modeling Selection Effects (Extra Credit!)\n",
    "Let's assume that our data has been generated with some non-trivial **selection function**, i.e. we are not observing the full population but instead some subset of it (almost always the case in astronomy!). How do we model this? A good example is in [Foreman-Mackey et al. 2014](https://ui.adsabs.harvard.edu/abs/2014ApJ...795...64F/abstract), Section 5.\n",
    "\n",
    "Here we will assume a detection efficiency of $Q_t = \\frac{n_{obs}}{n_{population}} = \\frac{1}{1+z}$, i.e. objects at higher redshift are harder to find (not unreasonable!). Implement this into our code above and see how our inferred population density changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-0YuhNWgvPe"
   },
   "outputs": [],
   "source": [
    "# Your work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjTPJHXFgvPe"
   },
   "source": [
    "# 8. Modeling Outliers (Extra Credit!)\n",
    "Let's assume that some (small) fraction of our data $f$ are _outliers_, i.e. the inferred redshifts are not consistent with our population model. Below we will step through how to model these by altering the likelihood.\n",
    "\n",
    "- Examine the likelihood for this so-called _mixture model_ in Section 3 of the [classic paper](https://ui.adsabs.harvard.edu/abs/2010arXiv1008.4686H/abstract) _Data analysis recipes: Fitting a model to data_ by David Hogg, Jo Bovy, and Dustin Lang.\n",
    "- Choose some fraction $f$ of your data to be outliers. To these objects, instead add $20\\sigma$ uncertainties.\n",
    "- Rewrite the likelihood function to include the outlier fraction $f$.\n",
    "- See if you can recover the outlier fraction by sampling the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoMOMnysgvPf"
   },
   "outputs": [],
   "source": [
    "# Your work here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "hierarchical_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
